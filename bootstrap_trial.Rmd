---
title: "Testing Bootstrap Method on Faux Data"
author: "Bracha Blau"
date: "February 15, 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

First we initilaize our data

```{r}
if (!require("pacman")){install.packages("pacman")}
pacman::p_load(GreedyExperimentalDesign, gurobi, designmatch)

n_vec = c(10, 30, 50)
p_vec = c(1, 5, 8)
beta_vec = c(c(1, 2, 4, 8, 16), rep(0, 10))
ALPHA = .05
NUM_ITERS = 100 #Number of trials to average power
NUM_ITERS_PER_TRIAL = 100 #Number of vectors to compute for each permutaion test/bootstrap test
TREATMENT_EFFECT = 30
SIGMA = 1
MAX_X = 100


#Setting up structure to hold avg power for both treatment effects.
avg_power_effect = list()
avg_power_effect$CR = matrix(nrow = length(n_vec), ncol = length(p_vec))
avg_power_effect$Greedy_search = matrix(nrow = length(n_vec), ncol = length(p_vec))
avg_power_effect$PSOD = matrix(nrow = length(n_vec), ncol = length(p_vec))

dimnames(avg_power_effect$CR) = list(n_vec, p_vec)
dimnames(avg_power_effect$Greedy_search) = list(n_vec, p_vec)
dimnames(avg_power_effect$PSOD) = list(n_vec, p_vec)

avg_power_null = list()
avg_power_null$CR = matrix(nrow = length(n_vec), ncol = length(p_vec))
avg_power_null$Greedy_search = matrix(nrow = length(n_vec), ncol = length(p_vec))
avg_power_null$PSOD = matrix(nrow = length(n_vec), ncol = length(p_vec))

dimnames(avg_power_null$CR) = list(n_vec, p_vec)
dimnames(avg_power_null$Greedy_search) = list(n_vec, p_vec)
dimnames(avg_power_null$PSOD) = list(n_vec, p_vec)



for(treatment_effect in c(TREATMENT_EFFECT, 0)){
  for(n in n_vec){
    for(p in p_vec){
      beta = beta_vec[1:p]
      rejections_vec = rep(0, 4) #vector to hold number of rejections for every method
      names(rejections_vec) = c("CR", "Greedy_search", "PSOD", "MSOD")
      cat("TESTING: n:",n," p:",p)
      for(iter in 1:NUM_ITERS){ #num of simulations for every condition
        start = Sys.time()
        
        X = matrix(runif(n*p, min =  0, max = MAX_X), nrow = n, ncol = p) #our fake data
        
        #Getting allocation vec under different methods and testing for power
        
        #Complete randomization
        alloc_vec_cr = cr(X)
        y_cr = get_yvec(X, alloc_vec_cr, beta, treatment_effect)
        rejections_vec["CR"] = rejections_vec["CR"] + run_hypothesis_test_cr(X, y_cr, alloc_vec_cr)
        rejections_vec["CR"]
        
        #Greedy search
        gs_obj = gs(X) #matrix of all the greedy search allocation vecs we'll need for this trial
        y_gs = get_yvec(X, gs_obj[, 1], beta, treatment_effect)
        rejections_vec["Greedy_search"] = rejections_vec["Greedy_search"] + run_hypothesis_test_gs(gs_obj[, 2:(NUM_ITERS_PER_TRIAL+1)], y_gs, gs_obj[,1])
        
        #PSOD
        #alloc_vec = psod(X)
        #y = get_yvec(X,allo_vec,beta)
        #rejections_vec["PSOD"] = rejections_vec["PSOD"] + run_bootstrap_test_psod
        
        #Pretty sure we will drop this test
        #MSOD
        #alloc_vec = msod(X)
        #y = get_yvec(X,allo_vec,beta)
        #rejections_vec("MSOD") = rejections_vec("MSOD") + test_power()
        endt = Sys.time()
        #cat("Time in one iteration: ", endt-start)
        
      }
      
      if(treatment_effect == TREATMENT_EFFECT){
        avg_power_effect$CR[toString(n), toString(p)] = rejections_vec["CR"]/NUM_ITERS
        avg_power_effect$Greedy_search[toString(n), toString(p)] = rejections_vec["Greedy_search"]/NUM_ITERS
        #avg_power_effect$PSOD[toString(n), toString(p)] = rejections_vec["PSOD"]/num_iters
      } else{
        avg_power_null$CR[toString(n), toString(p)] = rejections_vec["CR"]/NUM_ITERS
        avg_power_null$Greedy_search[toString(n), toString(p)] = rejections_vec["Greedy_search"]/NUM_ITERS
        #avg_power$PSOD[toString(n), toString(p)] = rejections_vec["PSOD"]/num_iters
      }
    }
  }
}


#Function to return y vector based on allocation vector
get_yvec = function(X, alloc_vec, beta, treatment_effect){
  epsilon = rnorm(nrow(X), 0, SIGMA)
  y = (X %*% beta) + (alloc_vec * treatment_effect) + epsilon
  
}

cr = function(X){
  sample(c(rep(0, nrow(X)/2), rep(1,nrow(X)/2)))
}

run_hypothesis_test_cr = function(X, y, alloc_vec){
  #initializing some vars
  #start_time = Sys.time()
  y_t = sum(y[which(alloc_vec == 1)])
  y_c = sum(y[which(alloc_vec == 0)])
  experimental_test_statistic = y_t - y_c
  test_statistics = array(NA, NUM_ITERS_PER_TRIAL)
  for(i in 1:NUM_ITERS_PER_TRIAL){ #comparing our test statistic to num_iters different allocation vectors
    allo_vec_test = cr(X)
    y_t_test = sum(y[which(allo_vec_test == 1)])
    y_c_test = sum(y[which(allo_vec_test == 0)])
    test_statistics[i] = y_t_test - y_c_test
  }
  
  ci_alpha = quantile(test_statistics, c(ALPHA / 2, 1- ALPHA / 2))
  #end_time = Sys.time()
  #cat("Time spent in CR hypo test: ",end_time-start_time)
  if (experimental_test_statistic <  ci_alpha[1] |  experimental_test_statistic > ci_alpha[2]) {
    #tally this is a reject
    1
  } else {
    #tally this is a retain
    0
  }
  
}
avg_power_effect
avg_power_null


gs = function(X){
  start = Sys.time()
  ged = initGreedyExperimentalDesignObject(X, 
        max_designs = 1 + NUM_ITERS_PER_TRIAL, 
        num_cores = 1, 
        wait = TRUE,
        objective = "mahal_dist")
  results = resultsGreedySearch(ged, max_vectors = 1 + NUM_ITERS_PER_TRIAL)
  #indicT_experiment = results$ending_indicTs[, 1, drop = FALSE]
  #indictTs_permutation_test = results$ending_indicTs[, 2 : (1 + NUM_PERM)]
  
  #results = resultsGreedySearch(ged, max_vectors = )
  en1 = Sys.time()
  #cat("Time getting gs obj: ",en1-start)
  results$ending_indicTs[, 1:(1+NUM_ITERS_PER_TRIAL), drop = FALSE]
}

run_hypothesis_test_gs = function(gs_vecs, y, alloc_vec){
  #start = Sys.time()
  #initializing some vars
  y_t = sum(y[which(alloc_vec == 1)])
  y_c = sum(y[which(alloc_vec == 0)])
  #cat("yt: ",y_t,"Y_c: ",y_c)
  experimental_test_statistic = y_t - y_c
  test_statistics = array(NA, NUM_ITERS_PER_TRIAL)
  for(i in 1:NUM_ITERS_PER_TRIAL){ #comparing our test statistic to num_iters different allocation vectors
    #cat("Greedt search allo vec# ",i," ;",gs_vecs[,i])
    allo_vec_test = as.numeric(gs_vecs[, i])
    y_t_test = sum(y[which(allo_vec_test == 1)])
    y_c_test = sum(y[which(allo_vec_test == 0)])
    #cat("For: ",i," y bar test: ",y_t_test," c bar perm: ",y_c_test)
    test_statistics[i] = y_t_test - y_c_test
  }
  
  ci_alpha = quantile(test_statistics, c(ALPHA / 2, 1- ALPHA / 2))
  #cat("exp test stat: ",experimental_test_statistic, "cialpah: ",ci_alpha)
  
  #endt = Sys.time()
  #cat("TIme in gs hypo: ", endt-start)

  if (experimental_test_statistic <  ci_alpha[1] |  experimental_test_statistic > ci_alpha[2]) {
    #tally this is a reject
    1
  } else {
    #tally this is a retain
    0
  }
  
}



psod = function(X){
  
}

run_bootstrap_test_psod = function(X){
  
}

msod = function(X){
  
}

```


My work from trying to min mahalnobis distance ignore for now
```{r}
pacman::p_load(ROI)

allo_vec = sample(c(rep(0,50),rep(1,50)))

dist = function(allo_vec){
  removeT = which(allo_vec == 1)
  removeC = which(allo_vec == 0)
  Xt = X[-removeC,]
  Xc = X[-removeT,]
  mahalanobis(Xt,Xc,cov = matrix(0,nrow=p,ncol=p))
}

objective = F_objective(F=dist,n=100)

op = OP(objective = objective, maximum = FALSE, types = B)

solution = ROI_solve(op)


```

